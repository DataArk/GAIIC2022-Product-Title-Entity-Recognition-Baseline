{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import os\n",
    "import jieba\n",
    "import torch\n",
    "import pickle\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "\n",
    "from ark_nlp.factory.utils.seed import set_seed \n",
    "from ark_nlp.model.ner.global_pointer_bert import GlobalPointerBert\n",
    "from ark_nlp.model.ner.global_pointer_bert import GlobalPointerBertConfig\n",
    "from ark_nlp.model.ner.global_pointer_bert import Dataset\n",
    "from ark_nlp.model.ner.global_pointer_bert import Task\n",
    "from ark_nlp.model.ner.global_pointer_bert import get_default_model_optimizer\n",
    "from ark_nlp.model.ner.global_pointer_bert import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 一、数据读入与处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. 数据读入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from ark_nlp.factory.utils.conlleval import get_entity_bio\n",
    "\n",
    "\n",
    "datalist = []\n",
    "with open('../data/source_datasets/train.txt', 'r', encoding='utf-8') as f:\n",
    "    lines = f.readlines()\n",
    "    lines.append('\\n')\n",
    "    \n",
    "    text = []\n",
    "    labels = []\n",
    "    label_set = set()\n",
    "    \n",
    "    for line in lines: \n",
    "        if line == '\\n':                \n",
    "            text = ''.join(text)\n",
    "            entity_labels = []\n",
    "            for _type, _start_idx, _end_idx in get_entity_bio(labels, id2label=None):\n",
    "                entity_labels.append({\n",
    "                    'start_idx': _start_idx,\n",
    "                    'end_idx': _end_idx,\n",
    "                    'type': _type,\n",
    "                    'entity': text[_start_idx: _end_idx+1]\n",
    "                })\n",
    "                \n",
    "            if text == '':\n",
    "                continue\n",
    "            \n",
    "            datalist.append({\n",
    "                'text': text,\n",
    "                'label': entity_labels\n",
    "            })\n",
    "            \n",
    "            text = []\n",
    "            labels = []\n",
    "            \n",
    "        elif line == '  O\\n':\n",
    "            text.append(' ')\n",
    "            labels.append('O')\n",
    "        else:\n",
    "            line = line.strip('\\n').split()\n",
    "            if len(line) == 1:\n",
    "                term = ' '\n",
    "                label = line[0]\n",
    "            else:\n",
    "                term, label = line\n",
    "            text.append(term)\n",
    "            label_set.add(label.split('-')[-1])\n",
    "            labels.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 这里随意分割了一下看指标，建议实际使用sklearn分割或者交叉验证\n",
    "\n",
    "train_data_df = pd.DataFrame(datalist)\n",
    "train_data_df['label'] = train_data_df['label'].apply(lambda x: str(x))\n",
    "\n",
    "dev_data_df = pd.DataFrame(datalist[-400:])\n",
    "dev_data_df['label'] = dev_data_df['label'].apply(lambda x: str(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_list = sorted(list(label_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_train_dataset = Dataset(train_data_df, categories=label_list)\n",
    "ner_dev_dataset = Dataset(dev_data_df, categories=ner_train_dataset.categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. 词典创建和生成分词器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(vocab='hfl/chinese-bert-wwm', max_seq_len=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. ID化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_train_dataset.convert_to_ids(tokenizer)\n",
    "ner_dev_dataset.convert_to_ids(tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### 二、模型构建"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. 模型参数设置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = GlobalPointerBertConfig.from_pretrained('hfl/chinese-bert-wwm', \n",
    "                                                 num_labels=len(ner_train_dataset.cat2id))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. 模型创建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl_module = GlobalPointerBert.from_pretrained('hfl/chinese-bert-wwm', \n",
    "                                              config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### 三、任务构建"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. 任务参数和必要部件设定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置运行次数\n",
    "num_epoches = 5\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = get_default_model_optimizer(dl_module)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. 任务创建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Task(dl_module, optimizer, 'gpce', cuda_device=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(ner_train_dataset, \n",
    "          ner_dev_dataset,\n",
    "          lr=2e-5,\n",
    "          epochs=num_epoches, \n",
    "          batch_size=batch_size\n",
    "         )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### 四、生成提交数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# ark-nlp提供该函数：from ark_nlp.model.ner.global_pointer_bert import Predictor\n",
    "# 这里主要是为了可以比较清晰地看到解码过程，所以将代码copy到这\n",
    "class GlobalPointerNERPredictor(object):\n",
    "    \"\"\"\n",
    "    GlobalPointer命名实体识别的预测器\n",
    "\n",
    "    Args:\n",
    "        module: 深度学习模型\n",
    "        tokernizer: 分词器\n",
    "        cat2id (:obj:`dict`): 标签映射\n",
    "    \"\"\"  # noqa: ignore flake8\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        module,\n",
    "        tokernizer,\n",
    "        cat2id\n",
    "    ):\n",
    "        self.module = module\n",
    "        self.module.task = 'TokenLevel'\n",
    "\n",
    "        self.cat2id = cat2id\n",
    "        self.tokenizer = tokernizer\n",
    "        self.device = list(self.module.parameters())[0].device\n",
    "\n",
    "        self.id2cat = {}\n",
    "        for cat_, idx_ in self.cat2id.items():\n",
    "            self.id2cat[idx_] = cat_\n",
    "\n",
    "    def _convert_to_transfomer_ids(\n",
    "        self,\n",
    "        text\n",
    "    ):\n",
    "\n",
    "        tokens = self.tokenizer.tokenize(text)\n",
    "        token_mapping = self.tokenizer.get_token_mapping(text, tokens)\n",
    "\n",
    "        input_ids = self.tokenizer.sequence_to_ids(tokens)\n",
    "        input_ids, input_mask, segment_ids = input_ids\n",
    "\n",
    "        zero = [0 for i in range(self.tokenizer.max_seq_len)]\n",
    "        span_mask = [input_mask for i in range(sum(input_mask))]\n",
    "        span_mask.extend([zero for i in range(sum(input_mask), self.tokenizer.max_seq_len)])\n",
    "        span_mask = np.array(span_mask)\n",
    "\n",
    "        features = {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': input_mask,\n",
    "            'token_type_ids': segment_ids,\n",
    "            'span_mask': span_mask\n",
    "        }\n",
    "\n",
    "        return features, token_mapping\n",
    "\n",
    "    def _get_input_ids(\n",
    "        self,\n",
    "        text\n",
    "    ):\n",
    "        if self.tokenizer.tokenizer_type == 'vanilla':\n",
    "            return self._convert_to_vanilla_ids(text)\n",
    "        elif self.tokenizer.tokenizer_type == 'transfomer':\n",
    "            return self._convert_to_transfomer_ids(text)\n",
    "        elif self.tokenizer.tokenizer_type == 'customized':\n",
    "            return self._convert_to_customized_ids(text)\n",
    "        else:\n",
    "            raise ValueError(\"The tokenizer type does not exist\")\n",
    "\n",
    "    def _get_module_one_sample_inputs(\n",
    "        self,\n",
    "        features\n",
    "    ):\n",
    "        return {col: torch.Tensor(features[col]).type(torch.long).unsqueeze(0).to(self.device) for col in features}\n",
    "\n",
    "    def predict_one_sample(\n",
    "        self,\n",
    "        text='',\n",
    "        threshold=0\n",
    "    ):\n",
    "        \"\"\"\n",
    "        单样本预测\n",
    "\n",
    "        Args:\n",
    "            text (:obj:`string`): 输入文本\n",
    "            threshold (:obj:`float`, optional, defaults to 0): 预测的阈值\n",
    "        \"\"\"  # noqa: ignore flake8\"\n",
    "\n",
    "        features, token_mapping = self._get_input_ids(text)\n",
    "        self.module.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            inputs = self._get_module_one_sample_inputs(features)\n",
    "            scores = self.module(**inputs)[0].cpu()\n",
    "            \n",
    "        scores[:, [0, -1]] -= np.inf\n",
    "        scores[:, :, [0, -1]] -= np.inf\n",
    "\n",
    "        entities = []\n",
    "\n",
    "        for category, start, end in zip(*np.where(scores > threshold)):\n",
    "            if end-1 > token_mapping[-1][-1]:\n",
    "                break\n",
    "            if token_mapping[start-1][0] <= token_mapping[end-1][-1]:\n",
    "                entitie_ = {\n",
    "                    \"start_idx\": token_mapping[start-1][0],\n",
    "                    \"end_idx\": token_mapping[end-1][-1],\n",
    "                    \"entity\": text[token_mapping[start-1][0]: token_mapping[end-1][-1]+1],\n",
    "                    \"type\": self.id2cat[category]\n",
    "                }\n",
    "\n",
    "                if entitie_['entity'] == '':\n",
    "                    continue\n",
    "\n",
    "                entities.append(entitie_)\n",
    "\n",
    "        return entities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_predictor_instance = GlobalPointerNERPredictor(model.module, tokenizer, ner_train_dataset.cat2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [14:00<00:00, 11.90it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "predict_results = []\n",
    "\n",
    "with open('../data/source_datasets/sample_per_line_preliminary_A.txt', 'r', encoding='utf-8') as f:\n",
    "    lines = f.readlines()\n",
    "    for _line in tqdm(lines):\n",
    "        label = len(_line) * ['O']\n",
    "        for _preditc in ner_predictor_instance.predict_one_sample(_line[:-1]):\n",
    "            if 'I' in label[_preditc['start_idx']]:\n",
    "                continue\n",
    "            if 'B' in label[_preditc['start_idx']] and 'O' not in label[_preditc['end_idx']]:\n",
    "                continue\n",
    "            if 'O' in label[_preditc['start_idx']] and 'B' in label[_preditc['end_idx']]:\n",
    "                continue\n",
    "\n",
    "            label[_preditc['start_idx']] = 'B-' +  _preditc['type']\n",
    "            label[_preditc['start_idx']+1: _preditc['end_idx']+1] = (_preditc['end_idx'] - _preditc['start_idx']) * [('I-' +  _preditc['type'])]\n",
    "            \n",
    "        predict_results.append([_line, label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('gobal_pointer_baseline.txt', 'w', encoding='utf-8') as f:\n",
    "    for _result in predict_results:\n",
    "        for word, tag in zip(_result[0], _result[1]):\n",
    "            if word == '\\n':\n",
    "                continue\n",
    "            f.write(f'{word} {tag}\\n')\n",
    "        f.write('\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
